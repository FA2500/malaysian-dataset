{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b252d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3.10 install -e . --no-deps\n",
    "# !pip3.10 install torchdiffeq x-transformers jieba pypinyin ema_pytorch\n",
    "# !wget https://gist.githubusercontent.com/huseinzol05/98974ae8c6c7a65d4bc0af9f5003786a/raw/2e06e71ef7349a57bc58cc9913ae6bae1f9f8447/mp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "508e7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.arrow_writer import ArrowWriter\n",
    "from datasets.arrow_reader import ArrowReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d570945f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/husein/ssd3/F5-TTS/src/f5_tts/../../data/Emilia_Malaysian_pinyin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from datasets.arrow_writer import ArrowWriter\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from f5_tts.model.utils import (\n",
    "    repetition_found,\n",
    "    convert_char_to_pinyin,\n",
    ")\n",
    "from importlib.resources import files\n",
    "\n",
    "en_filters = [\"ا\", \"い\", \"て\"]\n",
    "tokenizer = 'pinyin'\n",
    "dataset_name = f\"Emilia_Malaysian_{tokenizer}\"\n",
    "save_dir = str(files(\"f5_tts\").joinpath(\"../../\")) + f\"/data/{dataset_name}\"\n",
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aedfee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_audio_dir(file):\n",
    "    sub_result, durations = [], []\n",
    "    vocab_set = set()\n",
    "    \n",
    "    folder = os.path.split(file)[0]\n",
    "    filename = file.replace('.json', '')\n",
    "    \n",
    "    try:\n",
    "        with open(file) as fopen:\n",
    "            d = json.load(fopen)\n",
    "    except:\n",
    "        return sub_result, durations, vocab_set\n",
    "    \n",
    "    for no, obj in enumerate(d):\n",
    "        text = obj[\"text\"].strip()\n",
    "        if any(f in text for f in en_filters) or repetition_found(text, length=4):\n",
    "            continue\n",
    "        \n",
    "        if tokenizer == \"pinyin\":\n",
    "            text = convert_char_to_pinyin([text], polyphone=True)[0]\n",
    "        duration = obj[\"end\"] - obj['start']\n",
    "        audio_path = os.path.join(folder, f'{filename}_{no}.mp3')\n",
    "        sub_result.append({\"audio_path\": audio_path, \"text\": text, \"duration\": duration})\n",
    "        durations.append(duration)\n",
    "        vocab_set.update(list(text))\n",
    "    \n",
    "    return sub_result, durations, vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79667e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "def get_length(file):\n",
    "    process = subprocess.Popen(\n",
    "        ['ffmpeg', '-i', file],\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    "    )\n",
    "    stdout, stderr = process.communicate()\n",
    "    matches = re.search(\n",
    "        r\"Duration:\\s{1}(?P<hours>\\d+?):(?P<minutes>\\d+?):(?P<seconds>\\d+\\.\\d+?),\",\n",
    "        stdout.decode(),\n",
    "        re.DOTALL).groupdict()\n",
    "    return float(matches['hours']) * 60 * 60 + \\\n",
    "        float(matches['minutes']) * 60 + float(matches['seconds'])\n",
    "\n",
    "def loop(data):\n",
    "    sub_result, durations = [], []\n",
    "    vocab_set = set()\n",
    "    data, _ = data\n",
    "    for d in tqdm(data):\n",
    "        audio = os.path.join('/home/husein/ssd3', d['audio'])\n",
    "        duration = get_length(audio)\n",
    "        text = d[\"transcription\"].strip()\n",
    "        \n",
    "        if tokenizer == \"pinyin\":\n",
    "            text = convert_char_to_pinyin([text], polyphone=True)[0]\n",
    "            \n",
    "        sub_result.append({\"audio_path\": audio, \"text\": text, \"duration\": duration})\n",
    "        durations.append(duration)\n",
    "        vocab_set.update(list(text))\n",
    "        \n",
    "    return [[sub_result, durations, vocab_set]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e51fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet('/home/husein/ssd3/verify-text.parquet')\n",
    "data = data.to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eecc2582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': 'malaysian-podcast_processed_24k/Cara Nak Apply Student Exchange [vFhLEniT9X8]/Cara Nak Apply Student Exchange [vFhLEniT9X8]_0.mp3',\n",
       " 'transcription': 'Cara nak apply, macam Puteri kan time internship. So, Puteri punya keluar dekat group internship, aa, dia keluar satu form.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ce06d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop((data[:10], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "152f83fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:19:14<00:00, 14.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:19:14<00:00, 14.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:21:29<00:00, 14.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:10<00:00, 14.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:21<00:00, 14.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:29<00:00, 14.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:36<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:28<00:00, 14.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:37<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:33<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:35<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:35<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:54<00:00, 14.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:36<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:30<00:00, 14.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:37<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:42<00:00, 14.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:35<00:00, 14.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:44<00:00, 14.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 121911/121911 [2:22:43<00:00, 14.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import mp\n",
    "\n",
    "gather = mp.multiprocessing(data, loop, cores = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c791ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "duration_list = []\n",
    "text_vocab_set = set()\n",
    "\n",
    "for sub_result, durations, vocab_set in gather:\n",
    "    result.extend(sub_result)\n",
    "    duration_list.extend(durations)\n",
    "    text_vocab_set.update(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87cfee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{save_dir}\"):\n",
    "    os.makedirs(f\"{save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88429d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/SWivid/F5-TTS/resolve/main/F5TTS_Base/vocab.txt -O {save_dir}/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "160fb36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to raw.arrow ...: 100%|███████████████████████████████████████████████| 2438225/2438225 [00:19<00:00, 127594.78it/s]\n",
      "Writing to raw.arrow ...: 100%|███████████████████████████████████████████████| 2438225/2438225 [00:19<00:00, 124918.45it/s]\n"
     ]
    }
   ],
   "source": [
    "with ArrowWriter(path=f\"{save_dir}/raw-original.arrow\") as writer:\n",
    "    for line in tqdm(result, desc=\"Writing to raw.arrow ...\"):\n",
    "        writer.write(line)\n",
    "    \n",
    "with ArrowWriter(path=f\"{save_dir}/raw.arrow\") as writer:\n",
    "    for line in tqdm(result, desc=\"Writing to raw.arrow ...\"):\n",
    "        line['audio_path'] = line['audio_path'].replace('/home/husein/ssd3', '/workspace')\n",
    "        writer.write(line)\n",
    "        \n",
    "# dup a json separately saving duration in case for DynamicBatchSampler ease\n",
    "with open(f\"{save_dir}/duration.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"duration\": duration_list}, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6935a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as Dataset_\n",
    "dataset = Dataset_.from_file(f\"{save_dir}/raw.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa89fd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/malaysian-podcast_processed_24k/Cara Nak Apply Student Exchange [vFhLEniT9X8]/Cara Nak Apply Student Exchange [vFhLEniT9X8]_0.mp3'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['audio_path'].replace('/home/husein/ssd3', '/workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f79cbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f5_tts.model.dataset import load_dataset\n",
    "from f5_tts.model.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "87911e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_char_map, vocab_size = get_tokenizer('Emilia_Malaysian', 'pinyin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28ba16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_fft = 1024\n",
    "mel_spec_type = \"vocos\"  # 'vocos' or 'bigvgan'\n",
    "mel_spec_kwargs = dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0c28284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('Emilia_Malaysian', 'pinyin', mel_spec_kwargs=mel_spec_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb7b76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f5_tts.model.utils import list_str_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "effbb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_str_to_idx(train_dataset[0]['text'], vocab_char_map).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1fcfb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration.json  raw.arrow  raw-original.arrow  vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/Emilia_Malaysian_pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e75d987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcecc464dca4e198b1cd1fee76d10e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "duration.json:   0%|          | 0.00/15.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mesolitica/Malaysian-Voice-Conversion/commit/e4e3f8a78592db1a99eae1c7dc22f1be20159e18', commit_message='Upload data/Emilia_Malaysian_pinyin/duration.json with huggingface_hub', commit_description='', oid='e4e3f8a78592db1a99eae1c7dc22f1be20159e18', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"data/Emilia_Malaysian_pinyin/duration.json\",\n",
    "    path_in_repo=\"data/Emilia_Malaysian_pinyin/duration.json\",\n",
    "    repo_id=\"mesolitica/Malaysian-Voice-Conversion\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "410e480e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mesolitica/Malaysian-Voice-Conversion/commit/594784d5e2d28dd3cd838cf5709db83f82c44f64', commit_message='Upload data/Emilia_Malaysian_pinyin/vocab.txt with huggingface_hub', commit_description='', oid='594784d5e2d28dd3cd838cf5709db83f82c44f64', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"data/Emilia_Malaysian_pinyin/vocab.txt\",\n",
    "    path_in_repo=\"data/Emilia_Malaysian_pinyin/vocab.txt\",\n",
    "    repo_id=\"mesolitica/Malaysian-Voice-Conversion\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15fa40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7616137ed8064ec0802287611b415a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "raw.arrow:   0%|          | 0.00/2.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"data/Emilia_Malaysian_pinyin/raw.arrow\",\n",
    "    path_in_repo=\"data/Emilia_Malaysian_pinyin/raw.arrow\",\n",
    "    repo_id=\"mesolitica/Malaysian-Voice-Conversion\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e53c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
